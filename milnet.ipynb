{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from time import time\n",
    "from random import shuffle\n",
    "# !pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seg = 20\n",
    "max_word = 30\n",
    "\n",
    "train_amount = 400000\n",
    "test_amount = 200\n",
    "\n",
    "level_class_cnt = 5\n",
    "\n",
    "dropout_rate = 0.5\n",
    "hidden_feature_dim = 70\n",
    "gru_feature_dim = 150\n",
    "kernel_heights = [3, 4, 5]\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "w2v_weights_path = '/Volumes/CCsChunk2/datasets/nlp/milnet/nn_input/1/weights.npy'\n",
    "tensorboard_log_dir = '/Users/Frost/Desktop/log/'\n",
    "input_path = '/Volumes/CCsChunk2/datasets/nlp/milnet/nn_input/1/electronics.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = np.load(w2v_weights_path, allow_pickle=True)\n",
    "w2v_len = w2v.shape[1]\n",
    "\n",
    "fake_x = np.random.rand(1000, 20, 30)\n",
    "\n",
    "def label_map(raw_label):\n",
    "    if raw_label < 2:\n",
    "        return 0\n",
    "    elif raw_label < 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def data_generator(h5_input_path):\n",
    "    with h5py.File(h5_input_path) as in_file:\n",
    "        random_batch_order = [*range(len(in_file['document/'].keys()))]\n",
    "        shuffle(random_batch_order)\n",
    "        for index in random_batch_order:\n",
    "            doc, label = in_file['document/' + str(index)], in_file['label/' + str(index)]\n",
    "            random_doc_order = [*range(len(doc))]\n",
    "            shuffle(random_doc_order)\n",
    "            for i in random_doc_order:\n",
    "                yield np.array([doc[i].astype('float64')]), np.array([label_map(label[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Slice a piece from one dimension.\n",
    "\n",
    "The layer would slice the `index`th dimension from `target_dim` dimension of\n",
    "the input tensor, which have `total_dim` dimensions, then squeeze the tensor\n",
    "over the sliced dimension.\n",
    "\n",
    "Args:\n",
    "    total_dim (int): The total number of dimensions of the input tensor.\n",
    "    target_dim (int): The index of the dimension that need to slice.\n",
    "    index (int): The index of the dimension to keep in the slicing operation.\n",
    "\n",
    "Returns:\n",
    "    (Model): A keras model that implement the operation.\n",
    "'''\n",
    "def __get_filter_layer(total_dim, target_dim, index):\n",
    "    def tensor_filter(tensor_in):\n",
    "        nonlocal index\n",
    "        begin = [0 if i != target_dim else index for i in range(total_dim)]\n",
    "        size = [-1 if i != target_dim else 1 for i in range(total_dim)]\n",
    "        return tf.squeeze(tf.slice(tensor_in, begin, size), axis=target_dim)\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Lambda(tensor_filter)\n",
    "    ])\n",
    "\n",
    "\n",
    "''' Implement `submodel` for each slice of tensor.\n",
    "\n",
    "The model would slice its input tensor into pieces using `__get_filter_layer` \n",
    "along `branch_index`th dimension, then for each slice, implement submodel, \n",
    "finally the outputs of different submodels would be concated and reshaped to \n",
    "meet the demand of output.\n",
    "\n",
    "Args:\n",
    "    input_shape tuple(int): The shape of the input tensor.\n",
    "    branch_index (int): The index of the dimension to slice, start from 0 as \n",
    "        sample amount dimension.\n",
    "    output_shape tuple(int): The shape of the output tensor.\n",
    "    submodel (Model): The model to apply to different slices.\n",
    "    args (dict): The argument dictionary for `submodel`.\n",
    "'''\n",
    "def __get_branch_model(input_shape, branch_index, output_shape, submodel, args={}):\n",
    "    model_input = tf.keras.Input(input_shape)\n",
    "    sliced_inputs = [__get_filter_layer(len(input_shape) + 1, branch_index, i)(model_input) \n",
    "                     for i in range(input_shape[branch_index - 1])]\n",
    "    sub_instance = submodel(**args)\n",
    "    branch_models = [sub_instance(sliced_inputs[i]) \n",
    "                     for i in range(input_shape[branch_index - 1])]\n",
    "    concated_layers = tf.keras.layers.Concatenate()(branch_models)\n",
    "    model_output = tf.keras.layers.Reshape(output_shape)(concated_layers)\n",
    "    return tf.keras.Model(model_input, model_output)\n",
    "\n",
    "\n",
    "''' A CNN unit to encode segment with single kernel height.\n",
    "\n",
    "The unit would apply a convolution to its input to get a 2-dimensional \n",
    "tensor, then apply max overtime pooling to get a single dimensional tensor.\n",
    "\n",
    "Args:\n",
    "    input_shape ((int, int)): The shape of segment matrix. (word_max, w2v_len)\n",
    "    kernel_height (int): The height of the convolution kernel.\n",
    "    index (int): The index of the segment in its belonging document.\n",
    "\n",
    "Returns:\n",
    "    (Model): The CNN model to encode the segment matrix.\n",
    "'''\n",
    "def __get_sentence_encode_unit(input_shape, kernel_height):\n",
    "    global w2v_len\n",
    "    cnned_height = input_shape[0] - kernel_height + 1\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Reshape((*input_shape, 1)),\n",
    "        tf.keras.layers.Conv2D(hidden_feature_dim, (kernel_height, w2v_len)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Reshape((cnned_height, hidden_feature_dim, 1)),\n",
    "        tf.keras.layers.MaxPool2D((cnned_height, 1))\n",
    "    ])\n",
    "\n",
    "\n",
    "''' A CNN unit to encode segment with multiple kernel heights\n",
    "\n",
    "The unit would apply operation defined in `__get_sentence_encode_unit` for \n",
    "different kernel heights, then concat the result as a 1-dimensional tensor.\n",
    "\n",
    "Args:\n",
    "    input_shape ((int, int)): The shape of the document. (word_max, w2v_len)\n",
    "    kernel_heights ([int]): The list of the kernel heights.\n",
    "    index: The index of the segment in its belonging document.\n",
    "\n",
    "Returns:\n",
    "    (Model): The CNN model to encode the segment matrix.\n",
    "'''\n",
    "def __get_multi_kernel_encode_unit(input_shape, kernel_heights):\n",
    "    global w2v_len\n",
    "    model_input = tf.keras.Input(input_shape)\n",
    "    cnn_layers = [__get_sentence_encode_unit((input_shape), h)\n",
    "                     (model_input) for h in kernel_heights]\n",
    "    concated_layers = tf.keras.layers.concatenate(cnn_layers)\n",
    "    model_output = tf.keras.layers.Flatten()(concated_layers)\n",
    "    return tf.keras.Model(model_input, model_output)\n",
    "\n",
    "\n",
    "''' The softmax linear classifier for predicting segment sentiment.\n",
    "\n",
    "Returns:\n",
    "    (Model): The softmax linear classifier to predict segment sentiment.\n",
    "'''\n",
    "def __get_seg_classifier_unit():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(level_class_cnt, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "''' The unit to get the attention weight for a segment from hidden feature.\n",
    "\n",
    "Returns:\n",
    "    (Model): The model for predicting attention weight for a segment.\n",
    "\n",
    "'''\n",
    "def __get_attention_unit():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(2 * gru_feature_dim, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1, use_bias=False, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "''' A bidirectional-GRU unit to extract the hidden vectors.\n",
    "\n",
    "The hidden vectors are used to predict the attention weights of the model.\n",
    "\n",
    "Returns:\n",
    "    (Model): The bidirectional-GRU unit to predict the hidden vectors.\n",
    "'''\n",
    "def __get_bidirectional_gru_unit():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(gru_feature_dim, return_sequences=True)\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Model ...\n",
      "Model Constructed. Compiling ...\n",
      "Model Compiled.\n",
      "Model: \"model_84\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_91 (InputLayer)           [(None, 20, 30)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 20, 30, 300)  58749600    input_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_81 (Model)                (None, 20, 210)      253050      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_403 (Sequential)     (None, 20, 300)      325800      model_81[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_82 (Model)                (None, 20, 1)        90600       sequential_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "model_83 (Model)                (None, 20, 5)        1055        model_81[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 20, 5)        0           model_82[1][0]                   \n",
      "                                                                 model_83[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_462 (Lambda)             (None, 5)            0           multiply_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 59,420,105\n",
      "Trainable params: 670,085\n",
      "Non-trainable params: 58,750,020\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Constructing Model ...')\n",
    "\n",
    "model_input = tf.keras.Input((max_seg, max_word))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=w2v.shape[0], \n",
    "    output_dim=w2v_len, \n",
    "    weights=[w2v], \n",
    "    input_length=max_word, \n",
    "    trainable=False\n",
    ")(model_input)\n",
    "\n",
    "encoding_model = __get_branch_model(\n",
    "    input_shape=(max_seg, max_word, w2v_len), \n",
    "    branch_index=1, \n",
    "    output_shape=(max_seg, len(kernel_heights) * hidden_feature_dim), \n",
    "    submodel=__get_multi_kernel_encode_unit, \n",
    "    args={'kernel_heights': kernel_heights, 'input_shape': (max_word, w2v_len)}\n",
    ")(embedding_layer)\n",
    "\n",
    "biglu_model = __get_bidirectional_gru_unit()(encoding_model)\n",
    "\n",
    "attention_model = __get_branch_model(\n",
    "    input_shape=(max_seg, 2 * gru_feature_dim), \n",
    "    branch_index=1, \n",
    "    output_shape=(max_seg, 1), \n",
    "    submodel=__get_attention_unit\n",
    ")(biglu_model)\n",
    "\n",
    "classification_model = __get_branch_model(\n",
    "    input_shape=(max_seg, len(kernel_heights) * hidden_feature_dim), \n",
    "    branch_index=1, \n",
    "    output_shape=(max_seg, level_class_cnt), \n",
    "    submodel=__get_seg_classifier_unit\n",
    ")(encoding_model)\n",
    "\n",
    "weighted_layer = tf.keras.layers.Multiply()([attention_model, classification_model])\n",
    "\n",
    "reduce_layer = tf.keras.layers.Lambda(tf.reduce_mean, arguments={'axis': 1})(weighted_layer)\n",
    "\n",
    "model = tf.keras.Model(model_input, reduce_layer)\n",
    "\n",
    "print('Model Constructed. Compiling ...')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Model Compiled.')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0625 01:27:51.649326 140734800324032 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1251: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir + \"{}\".format(time()))\n",
    "\n",
    "model.fit_generator(\n",
    "    data_generator(input_path), \n",
    "    steps_per_epoch=train_amount // batch_size,\n",
    "    epochs=25,\n",
    "    callbacks=[tensorboard]\n",
    ")\n",
    "\n",
    "# model.predict(fake_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
