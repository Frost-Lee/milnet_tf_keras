{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import h5py\n",
    "from random import shuffle\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seg = 12\n",
    "max_word = 34\n",
    "level_class_cnt = 3\n",
    "test_percentage = 0.2\n",
    "\n",
    "dropout_rate = 0.5\n",
    "hidden_feature_dim = 70\n",
    "gru_feature_dim = 150\n",
    "kernel_heights = [3, 4, 5]\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 25\n",
    "\n",
    "w2v_weights_path = '/Volumes/CCsChunk2/datasets/nlp/milnet/nn_input/1/weights.npy'\n",
    "tensorboard_log_dir = '/Users/Frost/Desktop/log/'\n",
    "input_path = '/Volumes/CCsChunk2/datasets/nlp/milnet/nn_input/1/electronics.hdf5'\n",
    "\n",
    "sample_amount = 0\n",
    "mini_batch_cnt = 0\n",
    "with h5py.File(input_path) as in_file:\n",
    "    for index in range(len(in_file['label/'].keys())):\n",
    "        mini_batch_cnt += 1\n",
    "        sample_amount += len(in_file['label/' + str(index)])\n",
    "batch_indices = [*range(mini_batch_cnt)]\n",
    "shuffle(batch_indices)\n",
    "train_batches = batch_indices[0:int(mini_batch_cnt * (1 - test_percentage))]\n",
    "test_batches = batch_indices[int(mini_batch_cnt * (1 - test_percentage)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = np.load(w2v_weights_path, allow_pickle=True)\n",
    "w2v_len = w2v.shape[1]\n",
    "\n",
    "def label_map(raw_label):\n",
    "    if raw_label < 2:\n",
    "        return 0\n",
    "    elif raw_label < 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def data_generator(batch_indices, epochs=epochs):\n",
    "    global batch_size, input_path\n",
    "    with h5py.File(input_path) as in_file:\n",
    "        feature_array, label_array = np.zeros((batch_size, max_seg, max_word)), np.zeros((batch_size, 1))\n",
    "        batch_index = 0\n",
    "        for _ in range(epochs):\n",
    "            shuffle(batch_indices)\n",
    "            for index in batch_indices:\n",
    "                doc, label = in_file['document/' + str(index)], in_file['label/' + str(index)]\n",
    "                random_doc_order = [*range(len(doc))]\n",
    "                shuffle(random_doc_order)\n",
    "                for i in random_doc_order:\n",
    "                    feature_array[batch_index] = np.array([doc[i].astype('float64')])\n",
    "                    label_array[batch_index] = label_map(label[i])\n",
    "                    batch_index += 1\n",
    "                    if batch_index == batch_size:\n",
    "                        yield feature_array, label_array\n",
    "                        batch_index = 0\n",
    "                        feature_array, label_array = np.zeros((batch_size, max_seg, max_word)), np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Slice a piece from one dimension.\n",
    "\n",
    "The layer would slice the `index`th dimension from `target_dim` dimension of\n",
    "the input tensor, which have `total_dim` dimensions, then squeeze the tensor\n",
    "over the sliced dimension.\n",
    "\n",
    "Args:\n",
    "    total_dim (int): The total number of dimensions of the input tensor.\n",
    "    target_dim (int): The index of the dimension that need to slice.\n",
    "    index (int): The index of the dimension to keep in the slicing operation.\n",
    "\n",
    "Returns:\n",
    "    (Model): A keras model that implement the operation.\n",
    "'''\n",
    "def __get_filter_layer(total_dim, target_dim, index):\n",
    "    def tensor_filter(tensor_in):\n",
    "        nonlocal index\n",
    "        begin = [0 if i != target_dim else index for i in range(total_dim)]\n",
    "        size = [-1 if i != target_dim else 1 for i in range(total_dim)]\n",
    "        return tf.squeeze(tf.slice(tensor_in, begin, size), axis=target_dim)\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Lambda(tensor_filter)\n",
    "    ])\n",
    "\n",
    "\n",
    "''' Implement `submodel` for each slice of tensor.\n",
    "\n",
    "The model would slice its input tensor into pieces using `__get_filter_layer` \n",
    "along `branch_index`th dimension, then for each slice, implement submodel, \n",
    "finally the outputs of different submodels would be concated and reshaped to \n",
    "meet the demand of output.\n",
    "\n",
    "Args:\n",
    "    input_shape tuple(int): The shape of the input tensor.\n",
    "    branch_index (int): The index of the dimension to slice, start from 0 as \n",
    "        sample amount dimension.\n",
    "    output_shape tuple(int): The shape of the output tensor.\n",
    "    submodel (Model): The model to apply to different slices.\n",
    "    args (dict): The argument dictionary for `submodel`.\n",
    "'''\n",
    "def __get_branch_model(input_shape, branch_index, output_shape, submodel, args={}):\n",
    "    model_input = tf.keras.Input(input_shape)\n",
    "    sliced_inputs = [__get_filter_layer(len(input_shape) + 1, branch_index, i)(model_input) \n",
    "                     for i in range(input_shape[branch_index - 1])]\n",
    "    sub_instance = submodel(**args)\n",
    "    branch_models = [sub_instance(sliced_inputs[i]) \n",
    "                     for i in range(input_shape[branch_index - 1])]\n",
    "    concated_layers = tf.keras.layers.Concatenate()(branch_models)\n",
    "    model_output = tf.keras.layers.Reshape(output_shape)(concated_layers)\n",
    "    return tf.keras.Model(model_input, model_output)\n",
    "\n",
    "\n",
    "''' A CNN unit to encode segment with single kernel height.\n",
    "\n",
    "The unit would apply a convolution to its input to get a 2-dimensional \n",
    "tensor, then apply max overtime pooling to get a single dimensional tensor.\n",
    "\n",
    "Args:\n",
    "    input_shape ((int, int)): The shape of segment matrix. (word_max, w2v_len)\n",
    "    kernel_height (int): The height of the convolution kernel.\n",
    "    index (int): The index of the segment in its belonging document.\n",
    "\n",
    "Returns:\n",
    "    (Model): The CNN model to encode the segment matrix.\n",
    "'''\n",
    "def __get_sentence_encode_unit(input_shape, kernel_height):\n",
    "    global w2v_len\n",
    "    cnned_height = input_shape[0] - kernel_height + 1\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Reshape((*input_shape, 1)),\n",
    "        tf.keras.layers.Conv2D(hidden_feature_dim, (kernel_height, w2v_len)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Reshape((cnned_height, hidden_feature_dim, 1)),\n",
    "        tf.keras.layers.MaxPool2D((cnned_height, 1))\n",
    "    ])\n",
    "\n",
    "\n",
    "''' A CNN unit to encode segment with multiple kernel heights\n",
    "\n",
    "The unit would apply operation defined in `__get_sentence_encode_unit` for \n",
    "different kernel heights, then concat the result as a 1-dimensional tensor.\n",
    "\n",
    "Args:\n",
    "    input_shape ((int, int)): The shape of the document. (word_max, w2v_len)\n",
    "    kernel_heights ([int]): The list of the kernel heights.\n",
    "    index: The index of the segment in its belonging document.\n",
    "\n",
    "Returns:\n",
    "    (Model): The CNN model to encode the segment matrix.\n",
    "'''\n",
    "def __get_multi_kernel_encode_unit(input_shape, kernel_heights):\n",
    "    global w2v_len\n",
    "    model_input = tf.keras.Input(input_shape)\n",
    "    cnn_layers = [__get_sentence_encode_unit((input_shape), h)\n",
    "                     (model_input) for h in kernel_heights]\n",
    "    concated_layers = tf.keras.layers.concatenate(cnn_layers)\n",
    "    model_output = tf.keras.layers.Flatten()(concated_layers)\n",
    "    return tf.keras.Model(model_input, model_output)\n",
    "\n",
    "\n",
    "''' The softmax linear classifier for predicting segment sentiment.\n",
    "\n",
    "Returns:\n",
    "    (Model): The softmax linear classifier to predict segment sentiment.\n",
    "'''\n",
    "def __get_seg_classifier_unit():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(level_class_cnt, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "''' The unit to get the attention weight for a segment from hidden feature.\n",
    "\n",
    "Returns:\n",
    "    (Model): The model for predicting attention weight for a segment.\n",
    "\n",
    "'''\n",
    "def __get_attention_unit():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(2 * gru_feature_dim, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1, use_bias=False, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "''' A bidirectional-GRU unit to extract the hidden vectors.\n",
    "\n",
    "The hidden vectors are used to predict the attention weights of the model.\n",
    "\n",
    "Returns:\n",
    "    (Model): The bidirectional-GRU unit to predict the hidden vectors.\n",
    "'''\n",
    "def __get_bidirectional_gru_unit():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(gru_feature_dim, return_sequences=True)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "def performance_judge(model, generator, class_cnt=level_class_cnt):\n",
    "    eps = np.finfo(float).eps\n",
    "    accuracy, precisions, recalls, f1s = [], [], [], []\n",
    "    for i, (features, labels) in enumerate(generator):\n",
    "        predicted = model.predict(features)\n",
    "        precisions.append([])\n",
    "        recalls.append([])\n",
    "        f1s.append([])\n",
    "        contingency_table = np.zeros((class_cnt, class_cnt))\n",
    "        for index in range(features.shape[0]):\n",
    "            contingency_table[int(labels[index][0])][np.argmax(predicted[index])] += 1\n",
    "        accuracy.append(np.trace(contingency_table) / features.shape[0])\n",
    "        for index in range(class_cnt):\n",
    "            pass\n",
    "            precisions[i].append(contingency_table[index][index] / (np.sum(contingency_table[:, index]) + eps))\n",
    "            recalls[i].append(contingency_table[index][index] / (np.sum(contingency_table[index, :]) + eps))\n",
    "            f1s[i].append(2 * precisions[i][-1] * recalls[i][-1] / ((precisions[i][-1] + recalls[i][-1]) + eps))\n",
    "    precisions = [float(sum(l))/len(l) for l in zip(*precisions)]\n",
    "    recalls = [float(sum(l))/len(l) for l in zip(*recalls)]\n",
    "    f1s = [float(sum(l))/len(l) for l in zip(*f1s)]\n",
    "    print('Accuracy:', round(reduce(lambda x, y: x + y, accuracy) / len(accuracy), 3))\n",
    "    for index in range(class_cnt):\n",
    "        print('_____ Class', index, '_____')\n",
    "        print('Precision\\t', round(precisions[index], 3))\n",
    "        print('Recall\\t\\t', round(recalls[index], 3))\n",
    "        print('F1 Score\\t', round(f1s[index], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Model ...\n",
      "Model Constructed. Compiling ...\n",
      "Model Compiled.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 30)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 30, 300)  58749600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 20, 210)      253050      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 20, 300)      325800      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 20, 1)        90600       sequential_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 20, 5)        1055        model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 20, 5)        0           model_2[1][0]                    \n",
      "                                                                 model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 5)            0           multiply[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 59,420,105\n",
      "Trainable params: 670,085\n",
      "Non-trainable params: 58,750,020\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Constructing Model ...')\n",
    "\n",
    "model_input = tf.keras.Input((max_seg, max_word))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=w2v.shape[0], \n",
    "    output_dim=w2v_len, \n",
    "    weights=[w2v], \n",
    "    input_length=max_word, \n",
    "    trainable=False\n",
    ")(model_input)\n",
    "\n",
    "encoding_model = __get_branch_model(\n",
    "    input_shape=(max_seg, max_word, w2v_len), \n",
    "    branch_index=1, \n",
    "    output_shape=(max_seg, len(kernel_heights) * hidden_feature_dim), \n",
    "    submodel=__get_multi_kernel_encode_unit, \n",
    "    args={'kernel_heights': kernel_heights, 'input_shape': (max_word, w2v_len)}\n",
    ")(embedding_layer)\n",
    "\n",
    "biglu_model = __get_bidirectional_gru_unit()(encoding_model)\n",
    "\n",
    "attention_model = __get_branch_model(\n",
    "    input_shape=(max_seg, 2 * gru_feature_dim), \n",
    "    branch_index=1, \n",
    "    output_shape=(max_seg, 1), \n",
    "    submodel=__get_attention_unit\n",
    ")(biglu_model)\n",
    "\n",
    "classification_model = __get_branch_model(\n",
    "    input_shape=(max_seg, len(kernel_heights) * hidden_feature_dim), \n",
    "    branch_index=1, \n",
    "    output_shape=(max_seg, level_class_cnt), \n",
    "    submodel=__get_seg_classifier_unit\n",
    ")(encoding_model)\n",
    "\n",
    "weighted_layer = tf.keras.layers.Multiply()([attention_model, classification_model])\n",
    "\n",
    "reduce_layer = tf.keras.layers.Lambda(tf.reduce_mean, arguments={'axis': 1})(weighted_layer)\n",
    "\n",
    "model = tf.keras.Model(model_input, reduce_layer)\n",
    "\n",
    "print('Model Constructed. Compiling ...')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('Model Compiled.')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir + \"{}\".format(time()))\n",
    "\n",
    "model.fit_generator(\n",
    "    data_generator(train_batches), \n",
    "    steps_per_epoch=(sample_amount * (1 - test_percentage) // batch_size) - 1,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# model.save(model_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('############ Test Error ############')\n",
    "performance_judge(model, data_generator(test_batches, epochs=1))\n",
    "\n",
    "print('########## Training Error ##########')\n",
    "performance_judge(model, data_generator(train_batches, epochs=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
