{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import h5py\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from collections import deque\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_file_path = '/Users/Frost/Desktop/gourmet.txt'\n",
    "edu_file_path = '/Volumes/CCsChunk2/projects/nlp/nlp-lab-group9/data/processed/edus/electronics_edus.txt'\n",
    "w2v_path = '/Volumes/CCsChunk2/projects/nlp/milnet/embeddings/GoogleNews-vectors-negative300.bin'\n",
    "fasttext_path = '/Volumes/CCsChunk2/projects/nlp/milnet/embeddings/cc.en.300.vec'\n",
    "\n",
    "w2id_out_path = '/Volumes/CCsChunk2/projects/nlp/milnet/inputs/w2id.pkl'\n",
    "h5_out_path = '/Volumes/CCsChunk2/projects/nlp/milnet/inputs/electronics.hdf5'\n",
    "w2v_weights_out_path = '/Users/Frost/Desktop/weights.npy'\n",
    "\n",
    "document_out_dir = 'document/'\n",
    "label_out_dir = 'label/'\n",
    "\n",
    "stop_words_path = None\n",
    "\n",
    "w2v_vec_len = 300\n",
    "word_min_len = 2\n",
    "seg_min_len = 2\n",
    "seg_max_len = 50\n",
    "\n",
    "doc_min_seg = 1\n",
    "doc_max_seg = 20\n",
    "\n",
    "pad_value = 0\n",
    "\n",
    "batch_size = 200\n",
    "w2id_backup_batch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_re = re.compile('(\\d+)\\.\\d+')\n",
    "sentence_re = re.compile('(?:\\.|!|\\?)\\s')\n",
    "edu_re = re.compile('(?:\\.|!|\\?|(?:edu_break))\\s')\n",
    "\n",
    "multi_space_re = re.compile('\\s{2,}')\n",
    "url_re = re.compile('(http://)?www\\.[^ ]+')\n",
    "unknown_char_re = re.compile('[^a-z0-9$\\.\\?\\!\\'_]')\n",
    "multi_udl_re = re.compile('_{2,}')\n",
    "abram_re = re.compile('\\'m')\n",
    "abris_re = re.compile('\\'s')\n",
    "abrare_re = re.compile('\\'re')\n",
    "abrhave_re = re.compile('\\'ve')\n",
    "abrnot_re = re.compile('n\\'t')\n",
    "abrwd_re = re.compile('\\'d')\n",
    "abrwill_re = re.compile('\\'ll')\n",
    "num_re = re.compile('(?<= )[0-9]+(?= )')\n",
    "mixi_re = re.compile('(?<=[a-z])I')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "if stop_words_path is not None:\n",
    "    with open(stop_words_path) as in_file:\n",
    "        file_content = in_file.read()\n",
    "        stop_words = set(file_content.split('\\n'))\n",
    "lemmatizer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A dictionary to map words to their indices\n",
    "'''\n",
    "w2id = {}\n",
    "\n",
    "''' Truncate or pad the input vector so that their first dimension meets `target_len`\n",
    "\n",
    "Args:\n",
    "    input_vec (np.array): The input numpy array.\n",
    "    target_len (int): The expected length of the first dimension of the input vector.\n",
    "    pad_value (float): The value to pad the vector if the length is not enough. The value\n",
    "        padded will be numpy arrays which each value is `pad_value`.\n",
    "\n",
    "Returns:\n",
    "    (np.array): The processed `input_vec`.\n",
    "'''\n",
    "def __unilen_vec(input_vec, target_len, pad_value):\n",
    "    if input_vec.shape[0] > target_len:\n",
    "        return input_vec[0:target_len]\n",
    "    elif input_vec.shape[0] < target_len:\n",
    "        topad_len = target_len - input_vec.shape[0]\n",
    "        pad_width = [(0, 0) if i != 0 else (0, topad_len) for i in range(len(input_vec.shape))]\n",
    "        return np.pad(input_vec, pad_width, 'constant', constant_values=pad_value)\n",
    "    else:\n",
    "        return input_vec\n",
    "\n",
    "''' Convert a word to its id\n",
    "\n",
    "This method will use the global variable `w2id`, it will return the word's id if the word\n",
    "has already existed in the dictionary, or create a new id and return if the word is new.\n",
    "\n",
    "Args:\n",
    "    word (str): The input word.\n",
    "\n",
    "Returns:\n",
    "    (int): The id of the input `word`.\n",
    "'''\n",
    "def __word_to_id(word):\n",
    "    global w2id\n",
    "    if word in w2id:\n",
    "        return w2id[word]\n",
    "    else:\n",
    "        w2id[word] = len(w2id) + 1\n",
    "        return w2id[word]\n",
    "\n",
    "''' Clean a document\n",
    "\n",
    "This method will clean an input string (document). The cleaning includes replacing certain \n",
    "values with markers, expand abbreviation, remove stopwords (if required), lemmatize (if required), \n",
    "and merge spaces.\n",
    "\n",
    "Args:\n",
    "    raw_document (str): The input string (document).\n",
    "    stop_words (set(str)): The stop words set. If not `None`, words in this set will be removed.\n",
    "    lemmatizer (lemmatizer): The lemmatizer. If not `None`, each word will be lemmatized.\n",
    "\n",
    "Returns:\n",
    "    (str): The cleaned document.\n",
    "'''\n",
    "def __clean_document(raw_document, stop_words=None, lemmatizer=None):\n",
    "    raw_document = raw_document.lower()\n",
    "    raw_document = url_re.sub(' _url_', raw_document)\n",
    "    raw_document = multi_udl_re.sub('_', raw_document)\n",
    "    raw_document = abram_re.sub(' am', raw_document)\n",
    "    raw_document = abris_re.sub(' is', raw_document)\n",
    "    raw_document = abrare_re.sub(' are', raw_document)\n",
    "    raw_document = abrhave_re.sub(' have', raw_document)\n",
    "    raw_document = abrnot_re.sub(' not', raw_document)\n",
    "    raw_document = abrwd_re.sub(' would', raw_document)\n",
    "    raw_document = abrwill_re.sub(' will', raw_document)\n",
    "    raw_document = num_re.sub('<NUM>', raw_document)\n",
    "    raw_document = mixi_re.sub(' I', raw_document)\n",
    "    if lemmatizer is not None:\n",
    "        raw_document = ' '.join([w.lemma_ for w in lemmatizer(raw_document)])\n",
    "    if stop_words is not None:\n",
    "        raw_document = ' '.join([*filter(lambda x: x not in stop_words, raw_document.split(' '))])\n",
    "    raw_document = unknown_char_re.sub(' ', raw_document)\n",
    "    raw_document = multi_space_re.sub(' ', raw_document)\n",
    "    return raw_document\n",
    "\n",
    "''' Iterate the inputs and their labels in the input file\n",
    "\n",
    "This method will generate a tuple of the review and their label. In the file, label is supposed \n",
    "to be the first line, followed by several lines of review, then followed by an empty line.\n",
    "\n",
    "Args:\n",
    "    fp (file): The file pointer of the input file. Use `open` to open the file first.\n",
    "\n",
    "Returns:\n",
    "    (generator): The generator of the reviews of the file. Each yield returns a ((str), (str)) \n",
    "        tuple, which stand for (review, label).\n",
    "'''\n",
    "def load_labelled_document(fp):\n",
    "    doc_line_cnt = 0\n",
    "    label_cache = None\n",
    "    line_cache = None\n",
    "    for line in fp:\n",
    "        if line.strip() == '':\n",
    "            doc_line_cnt = 0\n",
    "            yield line_cache, label_cache\n",
    "            continue\n",
    "        if doc_line_cnt == 0:\n",
    "            label_cache = line\n",
    "        elif doc_line_cnt == 1:\n",
    "            line_cache = line\n",
    "        else:\n",
    "            line_cache += line\n",
    "        doc_line_cnt += 1\n",
    "\n",
    "''' Turn raw_segments into a list of cleaned word lists.\n",
    "\n",
    "Args:\n",
    "    raw_document (str): The input raw document.\n",
    "    segment_re (re): The compiled regex which identifies a segment breaker.\n",
    "    seg_min_len (int): The minimum length of a segment. Segments lower than this length will \n",
    "        be removed.\n",
    "    word_min_len (int): The minimum length of a word. Words lower than this length will be \n",
    "        removed.\n",
    "    stop_words (set(str)): The stop words set. If not `None`, words in this set will be removed.\n",
    "    lemmatizer (lemmatizer): The lemmatizer. If not `None`, each word will be lemmatized.\n",
    "\n",
    "Returns:\n",
    "    ([[str]]): A list of word lists for a review.\n",
    "'''\n",
    "def clean_split(raw_document, segment_re, seg_min_len, word_min_len, stop_words=None, lemmatizer=None):\n",
    "    cleaned_doc = __clean_document(raw_document, stop_words, lemmatizer)\n",
    "    seg_list = segment_re.split(cleaned_doc)\n",
    "    seg_list = [[*filter(lambda x: len(x) > word_min_len, seg.split(' '))] for seg in seg_list]\n",
    "    return [*filter(lambda x: len(x) > seg_min_len, seg_list)]\n",
    "\n",
    "''' Turn the cleaned list of word lists for a review into np.array.\n",
    "\n",
    "Args:\n",
    "    cleaned_segs ([[str]]): The cleaned list of word lists.\n",
    "    seg_max_len (int): The maximum length of a segment. Segments longer than this value will be \n",
    "        truncated.\n",
    "    doc_max_seg (int): The maximum length of a review. Reviews longer than this value will be \n",
    "        truncated.\n",
    "    pad_value (float): The value to pad the vector if the length is not enough. The value\n",
    "        padded will be numpy arrays which each value is `pad_value`.\n",
    "'''\n",
    "def doc_to_vec(cleaned_segs, doc_max_seg, seg_max_len, pad_value):\n",
    "    for index, seg in enumerate(cleaned_segs):\n",
    "        cleaned_segs[index] = np.array([*map(__word_to_id, seg)])\n",
    "        cleaned_segs[index] = __unilen_vec(cleaned_segs[index], seg_max_len, pad_value)\n",
    "    return __unilen_vec(np.array(cleaned_segs), doc_max_seg, pad_value)\n",
    "\n",
    "''' Get the label integer from input line.\n",
    "\n",
    "Args:\n",
    "    raw_label (str): The line containing label in the input file.\n",
    "\n",
    "Returns:\n",
    "    (int): The label.\n",
    "'''\n",
    "def clean_label(raw_label):\n",
    "    return int(label_re.findall(raw_label)[0]) - 1\n",
    "\n",
    "''' Dump a batch in a hdf5 file.\n",
    "\n",
    "'batch' refers to the batches in the hdf5 file, for reviews, the batches can be found at \n",
    "'<document_out_dir>/<batch_index>', for labels, the batches can be found at \n",
    "'<label_out_dir>/<batch_index>'.\n",
    "\n",
    "Args:\n",
    "    h5_out (h5py.File): The file handle of output file.\n",
    "    d_queue (deque(np.array)): The queue containing reviews. This will be cleared in the call.\n",
    "    l_queue (deque(np.array)): The queue containing labels. This will be cleared in the call.\n",
    "    batch_index (int): The current index of batch.\n",
    "    document_out_dir (str): The store location of the review batches.\n",
    "    label_out_dir (str): \n",
    "'''\n",
    "def dump_batch(h5_out, d_queue, l_queue, batch_index, document_out_dir, label_out_dir):\n",
    "    h5_out[document_out_dir + str(batch_index)] = np.array(d_queue)\n",
    "    d_queue.clear()\n",
    "    h5_out[label_out_dir + str(batch_index)] = np.array(l_queue)\n",
    "    l_queue.clear()\n",
    "\n",
    "''' A counter to summarize the length distribution of the collected lists.\n",
    "'''\n",
    "class Len_Counter():\n",
    "    '''\n",
    "    Args:\n",
    "        max_len (int): The maximum length of the collected lists. Lists exceed this amount will\n",
    "            be ignored.\n",
    "        description (str): The description of the functionality of this counter.\n",
    "    '''\n",
    "    def __init__(self, max_len, description=''):\n",
    "        self.len_dict = {l:0 for l in range(max_len)}\n",
    "        self.description = description\n",
    "    \n",
    "    ''' Add a new list to summarize.\n",
    "    Args:\n",
    "        x (list): The list to be added.\n",
    "    '''\n",
    "    def add(self, x):\n",
    "        if len(x) in self.len_dict:\n",
    "            self.len_dict[len(x)] += 1\n",
    "    \n",
    "    ''' Print the summary of the lists' length distribution.\n",
    "    Args:\n",
    "        checkpoints ([int]): The significance percentages. For each percentage p in `checkpoints`, this \n",
    "            method will output the length l of the list so that p percent of all lists' length is below l.\n",
    "    '''\n",
    "    def summary_distribution(self, checkpoints=[0.01, 0.05, 0.1, 0.2, 0.3, 0.5]):\n",
    "        print('\\n', self.description, 'summary:')\n",
    "        sorted_len = sorted([(k, v) for k, v in self.len_dict.items()], key=lambda x: x[0], reverse=True)\n",
    "        part_sum, sum_ptr = 0, 0\n",
    "        for checkpoint in sorted(checkpoints):\n",
    "            while part_sum / sum(v for k, v in sorted_len) < checkpoint:\n",
    "                part_sum += sorted_len[sum_ptr][1]\n",
    "                sum_ptr += 1\n",
    "            print('Length', sorted_len[sum_ptr][0], 'truncate ratio', checkpoint)\n",
    "\n",
    "''' Read w2v as dictionary from .bin file.\n",
    "\n",
    "Args:\n",
    "    bin_path (str): The path of .bin w2v file.\n",
    "\n",
    "Returns:\n",
    "    {str : np.array}: The w2v dictionary.\n",
    "'''\n",
    "def read_w2v(bin_path):\n",
    "    with open(bin_path, 'rb') as in_file:\n",
    "        w2v = {}\n",
    "        word_cnt, w2v_vec_len = map(int, in_file.readline().split())\n",
    "        bin_len = np.dtype('float32').itemsize * w2v_vec_len\n",
    "        for _ in range(word_cnt):\n",
    "            try:\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = in_file.read(1)\n",
    "                    if ch == b' ':\n",
    "                        word = ''.join(map(lambda x: x.decode('ascii'), word))\n",
    "                        break\n",
    "                    elif ch != '\\n':\n",
    "                        word.append(ch)\n",
    "                w2v[word] = np.fromstring(in_file.read(bin_len), dtype='float32')\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            if _ % (word_cnt / 100) == 0:\n",
    "                print('\\r w2v loading:', round(_ / word_cnt * 100, 0), end='%')\n",
    "        print('\\rLoading finished.')\n",
    "        return w2v\n",
    "\n",
    "''' Read fasttext as dictionary from text file.\n",
    "\n",
    "Args:\n",
    "    fasttext_path (str): The path of text fasttext file.\n",
    "\n",
    "Returns:\n",
    "    {str : np.array}: The fasttext dictionary.\n",
    "'''\n",
    "def read_fasttext(fasttext_path):\n",
    "    with open(fasttext_path) as in_file:\n",
    "        word_cnt, dimension = map(int, in_file.readline().split())\n",
    "        fasttext = {}\n",
    "        cnt = 0\n",
    "        for line in in_file:\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            fasttext[tokens[0]] = np.array([*map(float, tokens[1:])])\n",
    "            cnt += 1\n",
    "            if cnt % (word_cnt / 100) == 0:\n",
    "                print('\\r fasttext loading:', round(cnt / word_cnt * 100, 0), end='%')\n",
    "        return fasttext\n",
    "\n",
    "''' Dump the embedding weight with respect to `w2id`.\n",
    "\n",
    "This method will dump a npy file, in which the ith line is the w2v vector for word with id i.\n",
    "\n",
    "Args:\n",
    "    w2v ([str : np.array]): The w2v dictionary.\n",
    "    w2id ([str : int]): The dictionary mapping word to their id.\n",
    "    output_path (str): The path for dumping the npy weights file.\n",
    "'''\n",
    "def dump_embedding_weight(w2v, w2id, output_path):\n",
    "    weight_matrix = np.zeros((len(w2id) + 1, w2v_vec_len))\n",
    "    for word, index in w2id.items():\n",
    "        if word in w2v:\n",
    "            weight_matrix[index] = w2v[word]\n",
    "    weight_matrix.dump(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 407000 items processed.\n",
      " Segment Length summary:\n",
      "Length 15 truncate ratio 0.01\n",
      "Length 10 truncate ratio 0.05\n",
      "Length 9 truncate ratio 0.1\n",
      "Length 7 truncate ratio 0.2\n",
      "Length 6 truncate ratio 0.3\n",
      "Length 4 truncate ratio 0.5\n",
      "\n",
      " Document Length summary:\n",
      "Length 18 truncate ratio 0.01\n",
      "Length 16 truncate ratio 0.05\n",
      "Length 14 truncate ratio 0.1\n",
      "Length 11 truncate ratio 0.2\n",
      "Length 9 truncate ratio 0.3\n",
      "Length 6 truncate ratio 0.5\n",
      "\n",
      "Final dumping ...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "seg_len_counter, doc_len_counter = Len_Counter(seg_max_len, 'Segment Length'), Len_Counter(doc_max_seg, 'Document Length')\n",
    "\n",
    "with open(edu_file_path) as in_file, h5py.File(h5_out_path, 'w') as out_file:\n",
    "    document_queue = deque()\n",
    "    label_queue = deque()\n",
    "    cnt, batch_cnt = 0, 0\n",
    "    for document, label in load_labelled_document(in_file):\n",
    "        cleaned_label = clean_label(label)\n",
    "        cleaned_segs = clean_split(document, edu_re, seg_min_len, word_min_len, lemmatizer=lemmatizer)\n",
    "        if len(cleaned_segs) < doc_min_seg:\n",
    "            continue\n",
    "        for seg in cleaned_segs:\n",
    "            seg_len_counter.add(seg)\n",
    "        doc_len_counter.add(cleaned_segs)\n",
    "        document_queue.append(doc_to_vec(cleaned_segs, doc_max_seg, seg_max_len, pad_value))\n",
    "        label_queue.append(cleaned_label)\n",
    "        if cnt % batch_size == 0 and cnt != 0:\n",
    "            print('\\rDumping data ...', end='')\n",
    "            dump_batch(out_file, document_queue, label_queue, batch_cnt, document_out_dir, label_out_dir)\n",
    "            batch_cnt += 1\n",
    "        if cnt % w2id_backup_batch == 0 and cnt != 0:\n",
    "            print('\\rDumping w2id ...', end='')\n",
    "            with open(w2id_out_path, 'wb') as w2id_out:\n",
    "                pickle.dump(w2id, w2id_out)\n",
    "        if cnt % 100 == 0:\n",
    "            print('\\r', cnt, 'items processed.',end='')\n",
    "        cnt += 1\n",
    "    seg_len_counter.summary_distribution()\n",
    "    doc_len_counter.summary_distribution()\n",
    "    print('\\nFinal dumping ...')\n",
    "    dump_batch(out_file, document_queue, label_queue, batch_cnt, document_out_dir, label_out_dir)\n",
    "    with open(w2id_out_path, 'wb') as w2id_out:\n",
    "        pickle.dump(w2id, w2id_out)\n",
    "    print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:237: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished.0%\n"
     ]
    }
   ],
   "source": [
    "w2v = read_w2v(w2v_path)\n",
    "with open('/Volumes/CCsChunk2/projects/nlp/milnet/inputs/food_sentence/w2id.pkl', 'rb') as in_file:\n",
    "    w2id = pickle.load(in_file)\n",
    "dump_embedding_weight(w2v, w2id, w2v_weights_out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
