{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajB6Z4y2waff",
        "colab_type": "code",
        "outputId": "3e96947b-48a8-41c9-8116-8d6ffcfd3cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import time\n",
        "import h5py\n",
        "import operator\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from collections import deque\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orj2v7x4wdZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_file_path = '/content/gdrive/My Drive/data_source/milnet/preprocessing/electronics_sentences.txt'\n",
        "edu_file_path = '/content/gdrive/My Drive/data_source/milnet/preprocessing/electronics_edus.txt'\n",
        "w2v_path = '/content/gdrive/My Drive/data_source/milnet/preprocessing/w2v.pkl'\n",
        "h5_out_path = '/content/gdrive/My Drive/data_source/milnet/results/electronics.hdf5'\n",
        "w2id_out_path = '/content/gdrive/My Drive/data_source/milnet/results/w2id.pkl'\n",
        "weights_out_path = '/content/gdrive/My Drive/data_source/milnet/results/weights.npy'\n",
        "\n",
        "document_out_dir = 'document/'\n",
        "label_out_dir = 'label/'\n",
        "\n",
        "seglen_out_path = '/content/gdrive/My Drive/data_source/milnet/results/segment_summary/sentence_level/seglens.pkl'\n",
        "doclen_out_path = '/content/gdrive/My Drive/data_source/milnet/results/segment_summary/sentence_level/doclens.pkl'\n",
        "wordcnt_out_path = '/content/gdrive/My Drive/data_source/milnet/results/segment_summary/sentence_level/wordcnt.pkl'\n",
        "\n",
        "stop_words_path = None\n",
        "\n",
        "w2v_vec_len = 300\n",
        "word_min_len = 2\n",
        "seg_min_len = 2\n",
        "seg_max_len = 34\n",
        "\n",
        "doc_min_seg = 1\n",
        "doc_max_seg = 12\n",
        "\n",
        "pad_value = 0\n",
        "\n",
        "batch_size = 200\n",
        "w2id_backup_batch = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efetQfQawlAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_re = re.compile('(\\d+)\\.\\d+')\n",
        "sentence_re = re.compile('(?:\\.|!|\\?)\\s')\n",
        "edu_re = re.compile('(?:\\.|!|\\?|(?:EDU_BREAK))\\s')\n",
        "\n",
        "multi_space_re = re.compile('\\s{2,}')\n",
        "url_re = re.compile('(http://)?www\\.[^ ]+')\n",
        "unknown_char_re = re.compile('[^a-z0-9$\\'_]')\n",
        "multi_udl_re = re.compile('_{2,}')\n",
        "abram_re = re.compile('\\'m')\n",
        "abris_re = re.compile('\\'s')\n",
        "abrare_re = re.compile('\\'re')\n",
        "abrhave_re = re.compile('\\'ve')\n",
        "abrnot_re = re.compile('n\\'t')\n",
        "abrwd_re = re.compile('\\'d')\n",
        "abrwill_re = re.compile('\\'ll')\n",
        "num_re = re.compile('(?<= )[0-9]+(?= )')\n",
        "mixi_re = re.compile('(?<=[a-z])I')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "if stop_words_path is not None:\n",
        "    with open(stop_words_path) as in_file:\n",
        "        file_content = in_file.read()\n",
        "        stop_words = set(file_content.split('\\n'))\n",
        "lemmatizer = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMGXSwlZwwc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2id = {}\n",
        "\n",
        "def __unilen_vec(input_vec, target_len):\n",
        "    global pad_value\n",
        "    if input_vec.shape[0] > target_len:\n",
        "        return input_vec[0:target_len]\n",
        "    elif input_vec.shape[0] < target_len:\n",
        "        topad_len = target_len - input_vec.shape[0]\n",
        "        pad_width = [(0, 0) if i != 0 else (0, topad_len) for i in range(len(input_vec.shape))]\n",
        "        return np.pad(input_vec, pad_width, 'constant', constant_values=pad_value)\n",
        "    else:\n",
        "        return input_vec\n",
        "\n",
        "def __word_to_id(word):\n",
        "    global w2id\n",
        "    if word in w2id:\n",
        "        return w2id[word]\n",
        "    else:\n",
        "        w2id[word] = len(w2id) + 1\n",
        "        return w2id[word]\n",
        "\n",
        "def __clean_segment(raw_segment):\n",
        "    global word_min_len\n",
        "    raw_segment = raw_segment.lower()\n",
        "    raw_segment = url_re.sub(' _url_', raw_segment)\n",
        "    raw_segment = unknown_char_re.sub(' ', raw_segment)\n",
        "    raw_segment = multi_udl_re.sub('_', raw_segment)\n",
        "    raw_segment = abram_re.sub(' am', raw_segment)\n",
        "    raw_segment = abris_re.sub(' is', raw_segment)\n",
        "    raw_segment = abrare_re.sub(' are', raw_segment)\n",
        "    raw_segment = abrhave_re.sub(' have', raw_segment)\n",
        "    raw_segment = abrnot_re.sub(' not', raw_segment)\n",
        "    raw_segment = abrwd_re.sub(' would', raw_segment)\n",
        "    raw_segment = abrwill_re.sub(' will', raw_segment)\n",
        "    raw_segment = num_re.sub('<NUM>', raw_segment)\n",
        "    raw_segment = mixi_re.sub(' I', raw_segment)\n",
        "    raw_segment = multi_space_re.sub(' ', raw_segment)\n",
        "    return [*filter(lambda x: len(x) >= word_min_len, raw_segment.split(' '))]\n",
        "\n",
        "def load_labelled_document(fp):\n",
        "    doc_line_cnt = 0\n",
        "    label_cache = None\n",
        "    line_cache = None\n",
        "    for line in fp:\n",
        "        if line.strip() == '':\n",
        "            doc_line_cnt = 0\n",
        "            yield line_cache, label_cache\n",
        "            continue\n",
        "        if doc_line_cnt == 0:\n",
        "            label_cache = line\n",
        "        elif doc_line_cnt == 1:\n",
        "            line_cache = line\n",
        "        else:\n",
        "            line_cache += line\n",
        "        doc_line_cnt += 1\n",
        "\n",
        "def segment_document(raw_document, split_re):\n",
        "    seg_rst = split_re.split(raw_document)\n",
        "    return seg_rst\n",
        "\n",
        "def clean_segments(raw_segments, stopword_remove=True, lemmatize=True):\n",
        "    global seg_min_len, stop_words, lemmatizer\n",
        "    segments = [__clean_segment(seg) for seg in raw_segments]\n",
        "    if stopword_remove:\n",
        "        segments = [[*filter(lambda x: x not in stop_words)] for seg in segments]\n",
        "    if lemmatize:\n",
        "        len_lst = [len(seg) for seg in segments]\n",
        "        doc_words = ' '.join([' '.join(seg) for seg in segments])\n",
        "        lemmatized_words = [word.lemma_ for word in lemmatizer(doc_words)]\n",
        "        segments = []\n",
        "        for length in len_lst:\n",
        "            segments.append(lemmatized_words[:length])\n",
        "            lemmatized_words = lemmatized_words[length:]\n",
        "    return [*filter(lambda x: len(x) > seg_min_len, segments)]\n",
        "\n",
        "def doc_to_vec(cleaned_segs):\n",
        "    global seg_max_len, doc_max_seg\n",
        "    for index, seg in enumerate(cleaned_segs):\n",
        "        cleaned_segs[index] = np.array([*map(__word_to_id, seg)])\n",
        "        cleaned_segs[index] = __unilen_vec(cleaned_segs[index], seg_max_len)\n",
        "    return __unilen_vec(np.array(cleaned_segs), doc_max_seg)\n",
        "\n",
        "def clean_label(raw_label):\n",
        "    return int(label_re.findall(raw_label)[0]) - 1\n",
        "\n",
        "def dump_batch(h5_out, d_queue, l_queue, batch_index):\n",
        "    global document_out_dir, label_out_dir\n",
        "    h5_out[document_out_dir + str(batch_index)] = np.array(d_queue)\n",
        "    d_queue.clear()\n",
        "    h5_out[label_out_dir + str(batch_index)] = np.array(l_queue)\n",
        "    l_queue.clear()\n",
        "\n",
        "def dump_w2v(bin_path, output_path):\n",
        "    global w2v_vec_len\n",
        "    with open(bin_path, 'rb') as in_file:\n",
        "        w2v = {}\n",
        "        word_cnt, w2v_vec_len = map(int, in_file.readline().split())\n",
        "        bin_len = np.dtype('float32').itemsize * w2v_vec_len\n",
        "        for _ in range(word_cnt):\n",
        "            try:\n",
        "                word = []\n",
        "                while True:\n",
        "                    ch = in_file.read(1)\n",
        "                    if ch == b' ':\n",
        "                        word = ''.join(map(lambda x: x.decode('ascii'), word))\n",
        "                        break\n",
        "                    elif ch != '\\n':\n",
        "                        word.append(ch)\n",
        "                w2v[word] = np.fromstring(in_file.read(bin_len), dtype='float32')\n",
        "            except UnicodeDecodeError:\n",
        "                pass\n",
        "            if _ % (word_cnt / 100) == 0:\n",
        "                print('\\r', round(_ / word_cnt * 100, 0), end='')\n",
        "        print('Process finished, dumpting ...')\n",
        "        with open(output_path, 'wb') as out_file:\n",
        "            pickle.dump(w2v, out_file)\n",
        "\n",
        "def dump_embedding_weight(w2v, w2id, output_path):\n",
        "    weight_matrix = np.zeros((len(w2id) + 1, w2v_vec_len))\n",
        "    for word, index in w2id.items():\n",
        "        if word in w2v:\n",
        "            weight_matrix[index] = w2v[word]\n",
        "    weight_matrix.dump(output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naWc9p26w0Er",
        "colab_type": "code",
        "outputId": "f99083f3-973f-43ca-e103-f014baeafe0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_counter, seg_len_counter, doc_len_coutner = {}, {}, {}\n",
        "cnt = 0\n",
        "\n",
        "with open(sentence_file_path) as in_file:\n",
        "    for document, label in load_labelled_document(in_file):\n",
        "        cleaned_segs = clean_segments(segment_document(document, sentence_re), stopword_remove=False)\n",
        "        for segment in cleaned_segs:\n",
        "            if len(segment) in seg_len_counter:\n",
        "                seg_len_counter[len(segment)] += 1\n",
        "            else:\n",
        "                seg_len_counter[len(segment)] = 1\n",
        "            for word in segment:\n",
        "                if word in word_counter:\n",
        "                    word_counter[word] += 1\n",
        "                else:\n",
        "                    word_counter[word] = 1\n",
        "        if len(cleaned_segs) in doc_len_coutner:\n",
        "            doc_len_coutner[len(cleaned_segs)] += 1\n",
        "        else:\n",
        "            doc_len_coutner[len(cleaned_segs)] = 1\n",
        "        cnt += 1\n",
        "        if cnt % 100 == 0:\n",
        "            print('\\r', cnt, 'items processed.',end='')\n",
        "    with open(seglen_out_path, 'wb') as sl_out, open(doclen_out_path, 'wb') as dl_out, open(wordcnt_out_path, 'wb') as wc_out:\n",
        "        pickle.dump(seg_len_counter, sl_out)\n",
        "        pickle.dump(doc_len_coutner, dl_out)\n",
        "        pickle.dump(word_counter, wc_out)\n",
        "    print('Finished.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1200 items processed."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj017v2YVx-P",
        "colab_type": "code",
        "outputId": "e7714fe2-36a0-4c00-f291-e599b8ac3d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "with open(seglen_out_path, 'rb') as seg_in, open(doclen_out_path, 'rb') as doc_in:\n",
        "    seg_len_counter = pickle.load(seg_in)\n",
        "    doc_len_counter = pickle.load(doc_in)\n",
        "    sorted_seglen = sorted([(k, v) for k, v in seg_len_counter.items()], key=lambda x: x[0], reverse=True)\n",
        "    sorted_doclen = sorted([(k, v) for k, v in doc_len_counter.items()], key=lambda x: x[0], reverse=True)\n",
        "    checkpoint_percents = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
        "    seg_accum, doc_accum = 0, 0\n",
        "    seg_ptr, doc_ptr = 0, 0\n",
        "    for checkpoint in checkpoint_percents:\n",
        "        while seg_accum / sum(v for k, v in sorted_seglen) < checkpoint:\n",
        "            seg_accum += sorted_seglen[seg_ptr][1]\n",
        "            seg_ptr += 1\n",
        "        print('Segment len', sorted_seglen[seg_ptr][0], 'truncate ratio', checkpoint)\n",
        "    print('')\n",
        "    for checkpoint in checkpoint_percents:\n",
        "        while doc_accum / sum(v for k, v in sorted_doclen) < checkpoint:\n",
        "            doc_accum += sorted_doclen[doc_ptr][1]\n",
        "            doc_ptr += 1\n",
        "        print('Document len', sorted_doclen[doc_ptr][0], 'truncate ratio', checkpoint)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segment len 75 truncate ratio 0.01\n",
            "Segment len 44 truncate ratio 0.05\n",
            "Segment len 34 truncate ratio 0.1\n",
            "Segment len 25 truncate ratio 0.2\n",
            "Segment len 20 truncate ratio 0.3\n",
            "Segment len 14 truncate ratio 0.5\n",
            "\n",
            "Document len 31 truncate ratio 0.01\n",
            "Document len 17 truncate ratio 0.05\n",
            "Document len 12 truncate ratio 0.1\n",
            "Document len 8 truncate ratio 0.2\n",
            "Document len 6 truncate ratio 0.3\n",
            "Document len 3 truncate ratio 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNSz6fbox5jH",
        "colab_type": "code",
        "outputId": "570ae2f3-5906-454f-cac7-914f23d91171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "with open(edu_file_path) as in_file, h5py.File(h5_out_path, 'w') as out_file:\n",
        "    document_queue = deque()\n",
        "    label_queue = deque()\n",
        "    cnt, batch_cnt = 0, 0\n",
        "    for document, label in load_labelled_document(in_file):\n",
        "        cleaned_label = clean_label(label)\n",
        "        cleaned_segs = clean_segments(segment_document(document, edu_re), stopword_remove=False)\n",
        "        if len(cleaned_segs) < doc_min_seg:\n",
        "            continue\n",
        "        document_queue.append(doc_to_vec(cleaned_segs))\n",
        "        label_queue.append(cleaned_label)\n",
        "        if cnt % batch_size == 0 and cnt != 0:\n",
        "            print('\\rDumping data ...', end='')\n",
        "            dump_batch(out_file, document_queue, label_queue, batch_cnt)\n",
        "            batch_cnt += 1\n",
        "        if cnt % w2id_backup_batch == 0 and cnt != 0:\n",
        "            print('\\rDumping w2id ...', end='')\n",
        "            with open(w2id_out_path, 'wb') as w2id_out:\n",
        "                pickle.dump(w2id, w2id_out)\n",
        "        if cnt % 100 == 0:\n",
        "            print('\\r', cnt, 'items processed.',end='')\n",
        "        cnt += 1\n",
        "    print('\\rFinal dumping ...')\n",
        "    dump_batch(out_file, document_queue, label_queue, batch_cnt)\n",
        "    with open(w2id_out_path, 'wb') as w2id_out:\n",
        "        pickle.dump(w2id, w2id_out)\n",
        "    print('Finished.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final dumping ...\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr_-iFTtVCQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(w2v_path, 'rb') as in_file:\n",
        "    w2v = pickle.load(in_file)\n",
        "with open(w2id_out_path, 'rb') as in_file:\n",
        "    w2id = pickle.load(in_file)\n",
        "dump_embedding_weight(w2v, w2id, weights_out_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpAleuDbA313",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}