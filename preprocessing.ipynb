{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajB6Z4y2waff",
        "colab_type": "code",
        "outputId": "f21995c5-62a3-407c-8746-a29a9c3d32ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import time\n",
        "import h5py\n",
        "import operator\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from collections import deque\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orj2v7x4wdZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_file_path = '/content/gdrive/My Drive/data_source/milnet/preprocessing/electronics_sentences.txt'\n",
        "edu_file_path = '/content/gdrive/My Drive/data_source/milnet/preprocessing/electronics_edus.txt'\n",
        "w2v_path = '/content/gdrive/My Drive/data_source/milnet/preprocessing/w2v.pkl'\n",
        "\n",
        "h5_out_path = '/content/gdrive/My Drive/data_source/milnet/results/electronics.hdf5'\n",
        "w2id_out_path = '/content/gdrive/My Drive/data_source/milnet/results/w2id.pkl'\n",
        "weights_out_path = '/content/gdrive/My Drive/data_source/milnet/results/weights.npy'\n",
        "\n",
        "document_out_dir = 'document/'\n",
        "label_out_dir = 'label/'\n",
        "\n",
        "seglen_out_path = '/content/gdrive/My Drive/data_source/milnet/results/segment_summary/sentence_level/seglens.pkl'\n",
        "doclen_out_path = '/content/gdrive/My Drive/data_source/milnet/results/segment_summary/sentence_level/doclens.pkl'\n",
        "wordcnt_out_path = '/content/gdrive/My Drive/data_source/milnet/results/segment_summary/sentence_level/wordcnt.pkl'\n",
        "\n",
        "stop_words_path = None\n",
        "\n",
        "w2v_vec_len = 300\n",
        "word_min_len = 2\n",
        "seg_min_len = 2\n",
        "seg_max_len = 50\n",
        "\n",
        "doc_min_seg = 1\n",
        "doc_max_seg = 20\n",
        "\n",
        "pad_value = 0\n",
        "\n",
        "batch_size = 200\n",
        "w2id_backup_batch = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efetQfQawlAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_re = re.compile('(\\d+)\\.\\d+')\n",
        "sentence_re = re.compile('(?:\\.|!|\\?)\\s')\n",
        "edu_re = re.compile('(?:\\.|!|\\?|(?:EDU_BREAK))\\s')\n",
        "\n",
        "multi_space_re = re.compile('\\s{2,}')\n",
        "url_re = re.compile('(http://)?www\\.[^ ]+')\n",
        "unknown_char_re = re.compile('[^a-z0-9$\\'_]')\n",
        "multi_udl_re = re.compile('_{2,}')\n",
        "abram_re = re.compile('\\'m')\n",
        "abris_re = re.compile('\\'s')\n",
        "abrare_re = re.compile('\\'re')\n",
        "abrhave_re = re.compile('\\'ve')\n",
        "abrnot_re = re.compile('n\\'t')\n",
        "abrwd_re = re.compile('\\'d')\n",
        "abrwill_re = re.compile('\\'ll')\n",
        "num_re = re.compile('(?<= )[0-9]+(?= )')\n",
        "mixi_re = re.compile('(?<=[a-z])I')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "if stop_words_path is not None:\n",
        "    with open(stop_words_path) as in_file:\n",
        "        file_content = in_file.read()\n",
        "        stop_words = set(file_content.split('\\n'))\n",
        "lemmatizer = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMGXSwlZwwc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' A dictionary to map words to their indices\n",
        "'''\n",
        "w2id = {}\n",
        "\n",
        "''' Truncate or pad the input vector so that their first dimension meets `target_len`\n",
        "\n",
        "Args:\n",
        "    input_vec (np.array): The input numpy array.\n",
        "    target_len (int): The expected length of the first dimension of the input vector.\n",
        "    pad_value (float): The value to pad the vector if the length is not enough. The value\n",
        "        padded will be numpy arrays which each value is `pad_value`.\n",
        "\n",
        "Returns:\n",
        "    (np.array): The processed `input_vec`.\n",
        "'''\n",
        "def __unilen_vec(input_vec, target_len, pad_value):\n",
        "    if input_vec.shape[0] > target_len:\n",
        "        return input_vec[0:target_len]\n",
        "    elif input_vec.shape[0] < target_len:\n",
        "        topad_len = target_len - input_vec.shape[0]\n",
        "        pad_width = [(0, 0) if i != 0 else (0, topad_len) for i in range(len(input_vec.shape))]\n",
        "        return np.pad(input_vec, pad_width, 'constant', constant_values=pad_value)\n",
        "    else:\n",
        "        return input_vec\n",
        "\n",
        "''' Convert a word to its id\n",
        "\n",
        "This method will use the global variable `w2id`, it will return the word's id if the word\n",
        "has already existed in the dictionary, or create a new id and return if the word is new.\n",
        "\n",
        "Args:\n",
        "    word (str): The input word.\n",
        "\n",
        "Returns:\n",
        "    (int): The id of the input `word`.\n",
        "'''\n",
        "def __word_to_id(word):\n",
        "    global w2id\n",
        "    if word in w2id:\n",
        "        return w2id[word]\n",
        "    else:\n",
        "        w2id[word] = len(w2id) + 1\n",
        "        return w2id[word]\n",
        "\n",
        "''' Clean a segment\n",
        "\n",
        "This method will clean an input string (segment). The cleaning includes replacing certain \n",
        "values with markers, expand abbreviation, and merge spaces. Then the string will be splitted\n",
        "into word list.\n",
        "\n",
        "Args:\n",
        "    raw_setment (str): The input string (segment).\n",
        "    word_min_len (int): The minimum length of word in the word list. Words which length is below \n",
        "        this value will be removed.\n",
        "\n",
        "Returns:\n",
        "    ([str]): The cleaned word list.\n",
        "'''\n",
        "def __clean_segment(raw_segment, word_min_len):\n",
        "    raw_segment = raw_segment.lower()\n",
        "    raw_segment = url_re.sub(' _url_', raw_segment)\n",
        "    raw_segment = unknown_char_re.sub(' ', raw_segment)\n",
        "    raw_segment = multi_udl_re.sub('_', raw_segment)\n",
        "    raw_segment = abram_re.sub(' am', raw_segment)\n",
        "    raw_segment = abris_re.sub(' is', raw_segment)\n",
        "    raw_segment = abrare_re.sub(' are', raw_segment)\n",
        "    raw_segment = abrhave_re.sub(' have', raw_segment)\n",
        "    raw_segment = abrnot_re.sub(' not', raw_segment)\n",
        "    raw_segment = abrwd_re.sub(' would', raw_segment)\n",
        "    raw_segment = abrwill_re.sub(' will', raw_segment)\n",
        "    raw_segment = num_re.sub('<NUM>', raw_segment)\n",
        "    raw_segment = mixi_re.sub(' I', raw_segment)\n",
        "    raw_segment = multi_space_re.sub(' ', raw_segment)\n",
        "    return [*filter(lambda x: len(x) >= word_min_len, raw_segment.split(' '))]\n",
        "\n",
        "''' Iterate the inputs and their labels in the input file\n",
        "\n",
        "This method will generate a tuple of the review and their label. In the file, label is supposed \n",
        "to be the first line, followed by several lines of review, then followed by an empty line.\n",
        "\n",
        "Args:\n",
        "    fp (file): The file pointer of the input file. Use `open` to open the file first.\n",
        "\n",
        "Returns:\n",
        "    (generator): The generator of the reviews of the file. Each yield returns a ((str), (str)) \n",
        "        tuple, which stand for (review, label).\n",
        "'''\n",
        "def load_labelled_document(fp):\n",
        "    doc_line_cnt = 0\n",
        "    label_cache = None\n",
        "    line_cache = None\n",
        "    for line in fp:\n",
        "        if line.strip() == '':\n",
        "            doc_line_cnt = 0\n",
        "            yield line_cache, label_cache\n",
        "            continue\n",
        "        if doc_line_cnt == 0:\n",
        "            label_cache = line\n",
        "        elif doc_line_cnt == 1:\n",
        "            line_cache = line\n",
        "        else:\n",
        "            line_cache += line\n",
        "        doc_line_cnt += 1\n",
        "\n",
        "''' Segment the review according to some rules.\n",
        "\n",
        "Args:\n",
        "    raw_document (str): The input review.\n",
        "    split_re (re): The compiled regex which identifies a segment breaker.\n",
        "\n",
        "Returns:\n",
        "    ([str]): The segment list.\n",
        "'''\n",
        "def segment_document(raw_document, split_re):\n",
        "    seg_rst = split_re.split(raw_document)\n",
        "    return seg_rst\n",
        "\n",
        "''' Turn raw_segments into a list of cleaned word lists.\n",
        "\n",
        "Args:\n",
        "    raw_segments ([str]): The input raw segments.\n",
        "    seg_min_len (int): The minimum length of a segment. Segments lower than this length will \n",
        "        be removed.\n",
        "    word_min_len (int): The minimum length of a word. Words lower than this length will be \n",
        "        removed.\n",
        "    stop_words (set(str)): The stop words set. If not `None`, words in this set will be removed.\n",
        "    lemmatizer (lemmatizer): The lemmatizer. If not `None`, each word will be lemmatized.\n",
        "\n",
        "Returns:\n",
        "    ([[str]]): A list of word lists for a review.\n",
        "'''\n",
        "def clean_segments(raw_segments, seg_min_len, word_min_len, stop_words=None, lemmatizer=None):\n",
        "    segments = [__clean_segment(seg, word_min_len) for seg in raw_segments]\n",
        "    if stop_words != None:\n",
        "        segments = [[*filter(lambda x: x not in stop_words, seg)] for seg in segments]\n",
        "    if lemmatizer != None:\n",
        "        len_lst = [len(seg) for seg in segments]\n",
        "        doc_words = ' '.join([' '.join(seg) for seg in segments])\n",
        "        lemmatized_words = [word.lemma_ for word in lemmatizer(doc_words)]\n",
        "        segments = []\n",
        "        for length in len_lst:\n",
        "            segments.append(lemmatized_words[:length])\n",
        "            lemmatized_words = lemmatized_words[length:]\n",
        "    return [*filter(lambda x: len(x) > seg_min_len, segments)]\n",
        "\n",
        "''' Turn the cleaned list of word lists for a review into np.array.\n",
        "\n",
        "Args:\n",
        "    cleaned_segs ([[str]]): The cleaned list of word lists.\n",
        "    seg_max_len (int): The maximum length of a segment. Segments longer than this value will be \n",
        "        truncated.\n",
        "    doc_max_seg (int): The maximum length of a review. Reviews longer than this value will be \n",
        "        truncated.\n",
        "    pad_value (float): The value to pad the vector if the length is not enough. The value\n",
        "        padded will be numpy arrays which each value is `pad_value`.\n",
        "'''\n",
        "def doc_to_vec(cleaned_segs, doc_max_seg, seg_max_len, pad_value):\n",
        "    for index, seg in enumerate(cleaned_segs):\n",
        "        cleaned_segs[index] = np.array([*map(__word_to_id, seg)])\n",
        "        cleaned_segs[index] = __unilen_vec(cleaned_segs[index], seg_max_len, pad_value)\n",
        "    return __unilen_vec(np.array(cleaned_segs), doc_max_seg, pad_value)\n",
        "\n",
        "''' Get the label integer from input line.\n",
        "\n",
        "Args:\n",
        "    raw_label (str): The line containing label in the input file.\n",
        "\n",
        "Returns:\n",
        "    (int): The label.\n",
        "'''\n",
        "def clean_label(raw_label):\n",
        "    return int(label_re.findall(raw_label)[0]) - 1\n",
        "\n",
        "''' Dump a batch in a hdf5 file.\n",
        "\n",
        "'batch' refers to the batches in the hdf5 file, for reviews, the batches can be found at \n",
        "'<document_out_dir>/<batch_index>', for labels, the batches can be found at \n",
        "'<label_out_dir>/<batch_index>'.\n",
        "\n",
        "Args:\n",
        "    h5_out (h5py.File): The file handle of output file.\n",
        "    d_queue (deque(np.array)): The queue containing reviews. This will be cleared in the call.\n",
        "    l_queue (deque(np.array)): The queue containing labels. This will be cleared in the call.\n",
        "    batch_index (int): The current index of batch.\n",
        "    document_out_dir (str): The store location of the review batches.\n",
        "    label_out_dir (str): \n",
        "'''\n",
        "def dump_batch(h5_out, d_queue, l_queue, batch_index, document_out_dir, label_out_dir):\n",
        "    h5_out[document_out_dir + str(batch_index)] = np.array(d_queue)\n",
        "    d_queue.clear()\n",
        "    h5_out[label_out_dir + str(batch_index)] = np.array(l_queue)\n",
        "    l_queue.clear()\n",
        "\n",
        "''' A counter to summarize the length distribution of the collected lists.\n",
        "'''\n",
        "class Len_Counter():\n",
        "    '''\n",
        "    Args:\n",
        "        max_len (int): The maximum length of the collected lists. Lists exceed this amount will\n",
        "            be ignored.\n",
        "        description (str): The description of the functionality of this counter.\n",
        "    '''\n",
        "    def __init__(self, max_len, description=''):\n",
        "        self.len_dict = {l:0 for l in range(max_len)}\n",
        "        self.description = description\n",
        "    \n",
        "    ''' Add a new list to summarize.\n",
        "    Args:\n",
        "        x (list): The list to be added.\n",
        "    '''\n",
        "    def add(self, x):\n",
        "        if len(x) in self.len_dict:\n",
        "            self.len_dict[len(x)] += 1\n",
        "    \n",
        "    ''' Print the summary of the lists' length distribution.\n",
        "    Args:\n",
        "        checkpoints ([int]): The significance percentages. For each percentage p in `checkpoints`, this \n",
        "            method will output the length l of the list so that p percent of all lists' length is below l.\n",
        "    '''\n",
        "    def summary_distribution(self, checkpoints=[0.01, 0.05, 0.1, 0.2, 0.3, 0.5]):\n",
        "        print('\\n', self.description, 'summary:')\n",
        "        sorted_len = sorted([(k, v) for k, v in self.len_dict.items()], key=lambda x: x[0], reverse=True)\n",
        "        part_sum, sum_ptr = 0, 0\n",
        "        for checkpoint in sorted(checkpoints):\n",
        "            while part_sum / sum(v for k, v in sorted_len) < checkpoint:\n",
        "                part_sum += sorted_len[sum_ptr][1]\n",
        "                sum_ptr += 1\n",
        "            print('Length', sorted_len[sum_ptr][0], 'truncate ratio', checkpoint)\n",
        "\n",
        "''' Transform the .bin w2v file into pickle dictionary.\n",
        "\n",
        "The pickle dictionary will be string to np.array.\n",
        "\n",
        "Args:\n",
        "    bin_path (str): The path of .bin w2v file.\n",
        "    output_path (str): The output path of the dumped pickle file.\n",
        "    w2v_vec_len (int): The length of the w2v vector.\n",
        "'''\n",
        "def dump_w2v(bin_path, output_path, w2v_vec_len):\n",
        "    with open(bin_path, 'rb') as in_file:\n",
        "        w2v = {}\n",
        "        word_cnt, w2v_vec_len = map(int, in_file.readline().split())\n",
        "        bin_len = np.dtype('float32').itemsize * w2v_vec_len\n",
        "        for _ in range(word_cnt):\n",
        "            try:\n",
        "                word = []\n",
        "                while True:\n",
        "                    ch = in_file.read(1)\n",
        "                    if ch == b' ':\n",
        "                        word = ''.join(map(lambda x: x.decode('ascii'), word))\n",
        "                        break\n",
        "                    elif ch != '\\n':\n",
        "                        word.append(ch)\n",
        "                w2v[word] = np.fromstring(in_file.read(bin_len), dtype='float32')\n",
        "            except UnicodeDecodeError:\n",
        "                pass\n",
        "            if _ % (word_cnt / 100) == 0:\n",
        "                print('\\r', round(_ / word_cnt * 100, 0), end='')\n",
        "        print('Process finished, dumpting ...')\n",
        "        with open(output_path, 'wb') as out_file:\n",
        "            pickle.dump(w2v, out_file)\n",
        "\n",
        "''' Dump the embedding weight with respect to `w2id`.\n",
        "\n",
        "This method will dump a npy file, in which the ith line is the w2v vector for word with id i.\n",
        "\n",
        "Args:\n",
        "    w2v ([str : np.array]): The w2v dictionary.\n",
        "    w2id ([str : int]): The dictionary mapping word to their id.\n",
        "    output_path (str): The path for dumping the npy weights file.\n",
        "'''\n",
        "def dump_embedding_weight(w2v, w2id, output_path):\n",
        "    weight_matrix = np.zeros((len(w2id) + 1, w2v_vec_len))\n",
        "    for word, index in w2id.items():\n",
        "        if word in w2v:\n",
        "            weight_matrix[index] = w2v[word]\n",
        "    weight_matrix.dump(output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNSz6fbox5jH",
        "colab_type": "code",
        "outputId": "1a0b1e24-bf6c-4466-bfba-6e70149c0788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seg_len_counter, doc_len_counter = Len_Counter(seg_max_len, 'Segment Length'), Len_Counter(doc_max_seg, 'Document Length')\n",
        "\n",
        "with open(sentence_file_path) as in_file, h5py.File(h5_out_path, 'w') as out_file:\n",
        "    document_queue = deque()\n",
        "    label_queue = deque()\n",
        "    cnt, batch_cnt = 0, 0\n",
        "    for document, label in load_labelled_document(in_file):\n",
        "        cleaned_label = clean_label(label)\n",
        "        cleaned_segs = clean_segments(segment_document(document, sentence_re), seg_min_len, word_min_len, lemmatizer=lemmatizer)\n",
        "        if len(cleaned_segs) < doc_min_seg:\n",
        "            continue\n",
        "        for seg in cleaned_segs:\n",
        "            seg_len_counter.add(seg)\n",
        "        doc_len_counter.add(cleaned_segs)\n",
        "        document_queue.append(doc_to_vec(cleaned_segs, doc_max_seg, seg_max_len, pad_value))\n",
        "        label_queue.append(cleaned_label)\n",
        "        if cnt % batch_size == 0 and cnt != 0:\n",
        "            print('\\rDumping data ...', end='')\n",
        "            dump_batch(out_file, document_queue, label_queue, batch_cnt, document_out_dir, label_out_dir)\n",
        "            batch_cnt += 1\n",
        "        if cnt % w2id_backup_batch == 0 and cnt != 0:\n",
        "            print('\\rDumping w2id ...', end='')\n",
        "            with open(w2id_out_path, 'wb') as w2id_out:\n",
        "                pickle.dump(w2id, w2id_out)\n",
        "        if cnt % 100 == 0:\n",
        "            print('\\r', cnt, 'items processed.',end='')\n",
        "        cnt += 1\n",
        "    seg_len_counter.summary_distribution()\n",
        "    doc_len_counter.summary_distribution()\n",
        "    print('\\nFinal dumping ...')\n",
        "    dump_batch(out_file, document_queue, label_queue, batch_cnt, document_out_dir, label_out_dir)\n",
        "    with open(w2id_out_path, 'wb') as w2id_out:\n",
        "        pickle.dump(w2id, w2id_out)\n",
        "    print('Finished.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 15200 items processed."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr_-iFTtVCQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(w2v_path, 'rb') as in_file:\n",
        "    w2v = pickle.load(in_file)\n",
        "with open(w2id_out_path, 'rb') as in_file:\n",
        "    w2id = pickle.load(in_file)\n",
        "dump_embedding_weight(w2v, w2id, weights_out_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tms4moe2NhhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}